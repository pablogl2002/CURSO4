{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores, Learning Rate Schedulers\n",
    "\n",
    "En esta sesión vamos a comparar el resultado diferentes optimizadores y learning rates schedulers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer, normalizar y particionar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:02:30.212314: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-25 15:02:30.254313: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-25 15:02:30.450709: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-25 15:02:30.450736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-25 15:02:30.451873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-25 15:02:30.525014: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-25 15:02:30.525823: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-25 15:02:31.338377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set (60000, 28, 28)\n",
      "test set (10000, 28, 28)\n",
      "training set (48000, 784)\n",
      "val set (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "## Importar y normalizar datos\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train1, y_train1), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train1.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train1 = x_train1.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train1 = x_train1.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train1 /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train1 = keras.utils.to_categorical(y_train1, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train1, y_train1, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('val set', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores\n",
    "\n",
    "Vamos a probar diferentes optimizadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 1.2036 - accuracy: 0.7403 - val_loss: 0.6068 - val_accuracy: 0.8558\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.4885 - accuracy: 0.8780 - val_loss: 0.4130 - val_accuracy: 0.8896\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.3801 - accuracy: 0.8973 - val_loss: 0.3513 - val_accuracy: 0.9025\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.3341 - accuracy: 0.9074 - val_loss: 0.3153 - val_accuracy: 0.9120\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.3058 - accuracy: 0.9145 - val_loss: 0.2936 - val_accuracy: 0.9175\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 5s 11ms/step - loss: 0.2141 - accuracy: 0.9355 - val_loss: 0.1111 - val_accuracy: 0.9652\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0806 - accuracy: 0.9743 - val_loss: 0.1095 - val_accuracy: 0.9655\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0531 - accuracy: 0.9832 - val_loss: 0.0869 - val_accuracy: 0.9721\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 0.0927 - val_accuracy: 0.9745\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0306 - accuracy: 0.9902 - val_loss: 0.0781 - val_accuracy: 0.9791\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 1.8349 - accuracy: 0.5901 - val_loss: 1.3937 - val_accuracy: 0.7678\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 1.0794 - accuracy: 0.8048 - val_loss: 0.8492 - val_accuracy: 0.8267\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.7262 - accuracy: 0.8469 - val_loss: 0.6331 - val_accuracy: 0.8573\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.5768 - accuracy: 0.8664 - val_loss: 0.5288 - val_accuracy: 0.8712\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.4979 - accuracy: 0.8781 - val_loss: 0.4683 - val_accuracy: 0.8815\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "=============================\n",
      "Best acc 0.9790833592414856\n",
      "Best optim <keras.src.optimizers.adam.Adam object at 0x7fe2255d40d0>\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD,Adam,Adagrad\n",
    "\n",
    "batch_size=128\n",
    "epochs=5\n",
    "lr=0.001\n",
    "\n",
    "opt=[]\n",
    "opt.append(SGD(learning_rate=lr, momentum=0.9))\n",
    "opt.append(Adam(learning_rate=lr))\n",
    "opt.append(Adagrad(learning_rate=lr))\n",
    "\n",
    "best_acc=0.0\n",
    "for optim in opt:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(784))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    " \n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val)) \n",
    "    \n",
    "    print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "    \n",
    "    if history.history['val_accuracy'][-1]>best_acc:\n",
    "        best_acc=history.history['val_accuracy'][-1]\n",
    "        bestopt=optim\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"Best acc\",best_acc)\n",
    "print(\"Best optim\",bestopt)\n",
    "print(\"=============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIO**\n",
    "\n",
    "Añade más epochs a este ejemplo anterior, un early stopping y un model_checkpoint. Luego prueba el test sobre el model checkpoint almacenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 1.2512 - accuracy: 0.7348\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85817, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 1.2444 - accuracy: 0.7360 - val_loss: 0.6137 - val_accuracy: 0.8582\n",
      "Epoch 2/100\n",
      " 24/375 [>.............................] - ETA: 2s - loss: 0.6072 - accuracy: 0.8587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pegi/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/375 [============================>.] - ETA: 0s - loss: 0.4908 - accuracy: 0.8762\n",
      "Epoch 2: val_accuracy improved from 0.85817 to 0.89058, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.4890 - accuracy: 0.8765 - val_loss: 0.4140 - val_accuracy: 0.8906\n",
      "Epoch 3/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8969\n",
      "Epoch 3: val_accuracy improved from 0.89058 to 0.90458, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.3789 - accuracy: 0.8971 - val_loss: 0.3497 - val_accuracy: 0.9046\n",
      "Epoch 4/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9072\n",
      "Epoch 4: val_accuracy improved from 0.90458 to 0.91200, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.3331 - accuracy: 0.9071 - val_loss: 0.3157 - val_accuracy: 0.9120\n",
      "Epoch 5/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.9139\n",
      "Epoch 5: val_accuracy improved from 0.91200 to 0.91817, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.3052 - accuracy: 0.9141 - val_loss: 0.2939 - val_accuracy: 0.9182\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9197\n",
      "Epoch 6: val_accuracy improved from 0.91817 to 0.92167, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.2849 - accuracy: 0.9197 - val_loss: 0.2759 - val_accuracy: 0.9217\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9245\n",
      "Epoch 7: val_accuracy improved from 0.92167 to 0.92608, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.2682 - accuracy: 0.9245 - val_loss: 0.2626 - val_accuracy: 0.9261\n",
      "Epoch 8/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9282\n",
      "Epoch 8: val_accuracy improved from 0.92608 to 0.92942, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2546 - accuracy: 0.9282 - val_loss: 0.2511 - val_accuracy: 0.9294\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9321\n",
      "Epoch 9: val_accuracy improved from 0.92942 to 0.93083, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2425 - accuracy: 0.9321 - val_loss: 0.2441 - val_accuracy: 0.9308\n",
      "Epoch 10/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9355\n",
      "Epoch 10: val_accuracy improved from 0.93083 to 0.93567, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.2317 - accuracy: 0.9355 - val_loss: 0.2307 - val_accuracy: 0.9357\n",
      "Epoch 11/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9379\n",
      "Epoch 11: val_accuracy improved from 0.93567 to 0.93808, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2223 - accuracy: 0.9379 - val_loss: 0.2231 - val_accuracy: 0.9381\n",
      "Epoch 12/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9411\n",
      "Epoch 12: val_accuracy improved from 0.93808 to 0.93983, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2131 - accuracy: 0.9411 - val_loss: 0.2169 - val_accuracy: 0.9398\n",
      "Epoch 13/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9437\n",
      "Epoch 13: val_accuracy improved from 0.93983 to 0.94275, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2048 - accuracy: 0.9436 - val_loss: 0.2085 - val_accuracy: 0.9427\n",
      "Epoch 14/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1975 - accuracy: 0.9455\n",
      "Epoch 14: val_accuracy improved from 0.94275 to 0.94433, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1974 - accuracy: 0.9456 - val_loss: 0.2044 - val_accuracy: 0.9443\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9474\n",
      "Epoch 15: val_accuracy improved from 0.94433 to 0.94567, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1904 - accuracy: 0.9474 - val_loss: 0.1978 - val_accuracy: 0.9457\n",
      "Epoch 16/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9492\n",
      "Epoch 16: val_accuracy improved from 0.94567 to 0.94683, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1840 - accuracy: 0.9491 - val_loss: 0.1920 - val_accuracy: 0.9468\n",
      "Epoch 17/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9511\n",
      "Epoch 17: val_accuracy improved from 0.94683 to 0.94875, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.1777 - accuracy: 0.9510 - val_loss: 0.1862 - val_accuracy: 0.9488\n",
      "Epoch 18/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9528\n",
      "Epoch 18: val_accuracy improved from 0.94875 to 0.94925, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.1719 - accuracy: 0.9528 - val_loss: 0.1817 - val_accuracy: 0.9492\n",
      "Epoch 19/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9541\n",
      "Epoch 19: val_accuracy improved from 0.94925 to 0.95142, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1664 - accuracy: 0.9542 - val_loss: 0.1764 - val_accuracy: 0.9514\n",
      "Epoch 20/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1608 - accuracy: 0.9559\n",
      "Epoch 20: val_accuracy improved from 0.95142 to 0.95208, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1609 - accuracy: 0.9558 - val_loss: 0.1720 - val_accuracy: 0.9521\n",
      "Epoch 21/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9571\n",
      "Epoch 21: val_accuracy improved from 0.95208 to 0.95292, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1561 - accuracy: 0.9571 - val_loss: 0.1682 - val_accuracy: 0.9529\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9582\n",
      "Epoch 22: val_accuracy improved from 0.95292 to 0.95400, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1516 - accuracy: 0.9582 - val_loss: 0.1645 - val_accuracy: 0.9540\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9596\n",
      "Epoch 23: val_accuracy improved from 0.95400 to 0.95617, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.1471 - accuracy: 0.9596 - val_loss: 0.1609 - val_accuracy: 0.9562\n",
      "Epoch 24/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9608\n",
      "Epoch 24: val_accuracy improved from 0.95617 to 0.95658, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1427 - accuracy: 0.9608 - val_loss: 0.1573 - val_accuracy: 0.9566\n",
      "Epoch 25/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9616\n",
      "Epoch 25: val_accuracy improved from 0.95658 to 0.95750, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1387 - accuracy: 0.9616 - val_loss: 0.1543 - val_accuracy: 0.9575\n",
      "Epoch 26/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9628\n",
      "Epoch 26: val_accuracy improved from 0.95750 to 0.95875, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1350 - accuracy: 0.9628 - val_loss: 0.1511 - val_accuracy: 0.9588\n",
      "Epoch 27/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9638\n",
      "Epoch 27: val_accuracy did not improve from 0.95875\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1310 - accuracy: 0.9639 - val_loss: 0.1481 - val_accuracy: 0.9586\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch 1/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9352\n",
      "Epoch 1: val_accuracy improved from -inf to 0.96900, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.2146 - accuracy: 0.9353 - val_loss: 0.1050 - val_accuracy: 0.9690\n",
      "Epoch 2/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9751\n",
      "Epoch 2: val_accuracy improved from 0.96900 to 0.97292, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0775 - accuracy: 0.9751 - val_loss: 0.0919 - val_accuracy: 0.9729\n",
      "Epoch 3/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9847\n",
      "Epoch 3: val_accuracy improved from 0.97292 to 0.97508, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.0862 - val_accuracy: 0.9751\n",
      "Epoch 4/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9887\n",
      "Epoch 4: val_accuracy improved from 0.97508 to 0.97992, saving model to best_model.h5\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.0360 - accuracy: 0.9887 - val_loss: 0.0723 - val_accuracy: 0.9799\n",
      "Epoch 5/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9902\n",
      "Epoch 5: val_accuracy did not improve from 0.97992\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0292 - accuracy: 0.9902 - val_loss: 0.0911 - val_accuracy: 0.9756\n",
      "Epoch 6/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9927\n",
      "Epoch 6: val_accuracy did not improve from 0.97992\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.0822 - val_accuracy: 0.9799\n",
      "Epoch 7/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9944\n",
      "Epoch 7: val_accuracy did not improve from 0.97992\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.0963 - val_accuracy: 0.9773\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch 1/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.7753 - accuracy: 0.6410\n",
      "Epoch 1: val_accuracy improved from -inf to 0.78950, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 11ms/step - loss: 1.7753 - accuracy: 0.6410 - val_loss: 1.3203 - val_accuracy: 0.7895\n",
      "Epoch 2/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.8192\n",
      "Epoch 2: val_accuracy improved from 0.78950 to 0.83933, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 1.0292 - accuracy: 0.8195 - val_loss: 0.8178 - val_accuracy: 0.8393\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.8514\n",
      "Epoch 3: val_accuracy improved from 0.83933 to 0.86317, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.7052 - accuracy: 0.8514 - val_loss: 0.6187 - val_accuracy: 0.8632\n",
      "Epoch 4/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.5668 - accuracy: 0.8694\n",
      "Epoch 4: val_accuracy improved from 0.86317 to 0.87525, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5663 - accuracy: 0.8697 - val_loss: 0.5209 - val_accuracy: 0.8752\n",
      "Epoch 5/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.4921 - accuracy: 0.8800\n",
      "Epoch 5: val_accuracy improved from 0.87525 to 0.88533, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.4917 - accuracy: 0.8801 - val_loss: 0.4635 - val_accuracy: 0.8853\n",
      "Epoch 6/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.8873\n",
      "Epoch 6: val_accuracy improved from 0.88533 to 0.89242, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4452 - accuracy: 0.8871 - val_loss: 0.4250 - val_accuracy: 0.8924\n",
      "Epoch 7/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.4132 - accuracy: 0.8925\n",
      "Epoch 7: val_accuracy improved from 0.89242 to 0.89642, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4131 - accuracy: 0.8925 - val_loss: 0.3980 - val_accuracy: 0.8964\n",
      "Epoch 8/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8969\n",
      "Epoch 8: val_accuracy improved from 0.89642 to 0.90000, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3895 - accuracy: 0.8969 - val_loss: 0.3770 - val_accuracy: 0.9000\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.9005\n",
      "Epoch 9: val_accuracy improved from 0.90000 to 0.90375, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3712 - accuracy: 0.9005 - val_loss: 0.3601 - val_accuracy: 0.9038\n",
      "Epoch 10/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.9034\n",
      "Epoch 10: val_accuracy improved from 0.90375 to 0.90658, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3563 - accuracy: 0.9034 - val_loss: 0.3468 - val_accuracy: 0.9066\n",
      "Epoch 11/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.9060\n",
      "Epoch 11: val_accuracy improved from 0.90658 to 0.90950, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3438 - accuracy: 0.9062 - val_loss: 0.3356 - val_accuracy: 0.9095\n",
      "Epoch 12/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.9078\n",
      "Epoch 12: val_accuracy improved from 0.90950 to 0.91158, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3332 - accuracy: 0.9078 - val_loss: 0.3262 - val_accuracy: 0.9116\n",
      "Epoch 13/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9109\n",
      "Epoch 13: val_accuracy improved from 0.91158 to 0.91442, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3241 - accuracy: 0.9109 - val_loss: 0.3181 - val_accuracy: 0.9144\n",
      "Epoch 14/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9125\n",
      "Epoch 14: val_accuracy improved from 0.91442 to 0.91692, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3159 - accuracy: 0.9126 - val_loss: 0.3101 - val_accuracy: 0.9169\n",
      "Epoch 15/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.9145\n",
      "Epoch 15: val_accuracy improved from 0.91692 to 0.91833, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3084 - accuracy: 0.9145 - val_loss: 0.3034 - val_accuracy: 0.9183\n",
      "Epoch 16/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.9159\n",
      "Epoch 16: val_accuracy improved from 0.91833 to 0.91900, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3018 - accuracy: 0.9160 - val_loss: 0.2971 - val_accuracy: 0.9190\n",
      "Epoch 17/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.9177\n",
      "Epoch 17: val_accuracy improved from 0.91900 to 0.92017, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2956 - accuracy: 0.9176 - val_loss: 0.2919 - val_accuracy: 0.9202\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9196\n",
      "Epoch 18: val_accuracy improved from 0.92017 to 0.92108, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2900 - accuracy: 0.9196 - val_loss: 0.2864 - val_accuracy: 0.9211\n",
      "Epoch 19/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9211\n",
      "Epoch 19: val_accuracy improved from 0.92108 to 0.92283, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2847 - accuracy: 0.9210 - val_loss: 0.2820 - val_accuracy: 0.9228\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9222\n",
      "Epoch 20: val_accuracy did not improve from 0.92283\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2797 - accuracy: 0.9222 - val_loss: 0.2777 - val_accuracy: 0.9227\n",
      "Epoch 21/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.9235\n",
      "Epoch 21: val_accuracy improved from 0.92283 to 0.92417, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.2751 - accuracy: 0.9236 - val_loss: 0.2730 - val_accuracy: 0.9242\n",
      "Epoch 22/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.9248\n",
      "Epoch 22: val_accuracy improved from 0.92417 to 0.92533, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.2707 - accuracy: 0.9247 - val_loss: 0.2695 - val_accuracy: 0.9253\n",
      "Epoch 23/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.9257\n",
      "Epoch 23: val_accuracy improved from 0.92533 to 0.92667, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2664 - accuracy: 0.9258 - val_loss: 0.2652 - val_accuracy: 0.9267\n",
      "Epoch 24/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2626 - accuracy: 0.9273\n",
      "Epoch 24: val_accuracy improved from 0.92667 to 0.92733, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2624 - accuracy: 0.9274 - val_loss: 0.2618 - val_accuracy: 0.9273\n",
      "Epoch 25/100\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9278\n",
      "Epoch 25: val_accuracy improved from 0.92733 to 0.92842, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.2586 - accuracy: 0.9279 - val_loss: 0.2584 - val_accuracy: 0.9284\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9292\n",
      "Epoch 26: val_accuracy improved from 0.92842 to 0.92942, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.2549 - accuracy: 0.9292 - val_loss: 0.2552 - val_accuracy: 0.9294\n",
      "Epoch 27/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.9305\n",
      "Epoch 27: val_accuracy improved from 0.92942 to 0.92950, saving model to best_model.h5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.2515 - accuracy: 0.9304 - val_loss: 0.2520 - val_accuracy: 0.9295\n",
      "Epoch 28/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9312\n",
      "Epoch 28: val_accuracy improved from 0.92950 to 0.93150, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2481 - accuracy: 0.9312 - val_loss: 0.2488 - val_accuracy: 0.9315\n",
      "Epoch 29/100\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9323\n",
      "Epoch 29: val_accuracy improved from 0.93150 to 0.93217, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2448 - accuracy: 0.9321 - val_loss: 0.2462 - val_accuracy: 0.9322\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "=============================\n",
      "Best acc 0.9773333072662354\n",
      "Best optim <keras.src.optimizers.adam.Adam object at 0x7f4fc759c0d0>\n",
      "=============================\n",
      "Test loss: 0.23786430060863495\n",
      "Test accuracy: 0.9333000183105469\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD,Adam,Adagrad\n",
    "\n",
    "batch_size=128\n",
    "epochs=100\n",
    "lr=0.001\n",
    "\n",
    "opt=[]\n",
    "opt.append(SGD(learning_rate=lr, momentum=0.9))\n",
    "opt.append(Adam(learning_rate=lr))\n",
    "opt.append(Adagrad(learning_rate=lr))\n",
    "\n",
    "best_acc=0.0\n",
    "for optim in opt:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(784))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[callback, checkpoint]) \n",
    "    \n",
    "    print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "    \n",
    "    if history.history['val_accuracy'][-1]>best_acc:\n",
    "        best_acc=history.history['val_accuracy'][-1]\n",
    "        bestopt=optim\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"Best acc\",best_acc)\n",
    "print(\"Best optim\",bestopt)\n",
    "print(\"=============================\")\n",
    "\n",
    "\n",
    "## cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rates Schedulers\n",
    "\n",
    "Los learning rate schedulers son mecanismos de modificación del learning rate. Normalmente lo que hacen es reducir el valor del learning rate, lo que se conoce como ***learning rate annealing***. Esta modificación se suele realizar al acabar cada epoch.\n",
    "\n",
    "Keras ya dispone de algunos learning rate schedulers implementados pero el usuario puede implemetar su propia estrategia de annealing. Veamos ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS ya implementado en Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3721 - accuracy: 0.9089 - val_loss: 0.2271 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.1643 - accuracy: 0.9516 - val_loss: 0.1491 - val_accuracy: 0.9587 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.1295 - accuracy: 0.9632 - val_loss: 0.1477 - val_accuracy: 0.9577 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1127 - accuracy: 0.9683 - val_loss: 0.1808 - val_accuracy: 0.9563 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.1063 - accuracy: 0.9700 - val_loss: 0.1621 - val_accuracy: 0.9596 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Emplear un LRS estandard de Keras: ReduceLROnPlateau\n",
    "epochs = 5\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=2, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr])  ## <--- aquí está\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio mediante función \n",
    "\n",
    "La función toma como entrada el epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2116 - accuracy: 0.9365 - val_loss: 0.0971 - val_accuracy: 0.9716 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0800 - accuracy: 0.9751 - val_loss: 0.0932 - val_accuracy: 0.9718 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0480 - accuracy: 0.9844 - val_loss: 0.0779 - val_accuracy: 0.9777 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0346 - accuracy: 0.9890 - val_loss: 0.0852 - val_accuracy: 0.9777 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0284 - accuracy: 0.9903 - val_loss: 0.0964 - val_accuracy: 0.9768 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "## Emplear un LRS propio: LearningRateScheduler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "LRS=LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])   \n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio tipo CosineAnnealing\n",
    "\n",
    "![cosine annealing](cosine.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2125 - accuracy: 0.9356 - val_loss: 0.1679 - val_accuracy: 0.9468 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0784 - accuracy: 0.9755 - val_loss: 0.1005 - val_accuracy: 0.9697 - lr: 9.9975e-04\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0537 - accuracy: 0.9826 - val_loss: 0.0935 - val_accuracy: 0.9718 - lr: 9.9901e-04\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0354 - accuracy: 0.9884 - val_loss: 0.0918 - val_accuracy: 0.9736 - lr: 9.9778e-04\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0262 - accuracy: 0.9916 - val_loss: 0.0875 - val_accuracy: 0.9776 - lr: 9.9606e-04\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0239 - accuracy: 0.9919 - val_loss: 0.0819 - val_accuracy: 0.9787 - lr: 9.9384e-04\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0241 - accuracy: 0.9916 - val_loss: 0.0798 - val_accuracy: 0.9798 - lr: 9.9114e-04\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.0928 - val_accuracy: 0.9795 - lr: 9.8796e-04\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0878 - val_accuracy: 0.9808 - lr: 9.8429e-04\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.0885 - val_accuracy: 0.9807 - lr: 9.8015e-04\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.1049 - val_accuracy: 0.9799 - lr: 9.7553e-04\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.1066 - val_accuracy: 0.9789 - lr: 9.7044e-04\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0136 - accuracy: 0.9963 - val_loss: 0.1091 - val_accuracy: 0.9787 - lr: 9.6489e-04\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.1175 - val_accuracy: 0.9786 - lr: 9.5888e-04\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.1025 - val_accuracy: 0.9808 - lr: 9.5241e-04\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.1150 - val_accuracy: 0.9805 - lr: 9.4550e-04\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 0.1095 - val_accuracy: 0.9796 - lr: 9.3815e-04\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0997 - val_accuracy: 0.9830 - lr: 9.3037e-04\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.1012 - val_accuracy: 0.9829 - lr: 9.2216e-04\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.1014 - val_accuracy: 0.9837 - lr: 9.1354e-04\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.1264 - val_accuracy: 0.9781 - lr: 9.0451e-04\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.1271 - val_accuracy: 0.9816 - lr: 8.9508e-04\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.1137 - val_accuracy: 0.9818 - lr: 8.8526e-04\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.1113 - val_accuracy: 0.9828 - lr: 8.7506e-04\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1277 - val_accuracy: 0.9818 - lr: 8.6448e-04\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.1318 - val_accuracy: 0.9805 - lr: 8.5355e-04\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.1162 - val_accuracy: 0.9824 - lr: 8.4227e-04\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.1107 - val_accuracy: 0.9832 - lr: 8.3066e-04\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0967 - val_accuracy: 0.9857 - lr: 8.1871e-04\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 7.1388e-04 - accuracy: 0.9999 - val_loss: 0.1132 - val_accuracy: 0.9852 - lr: 8.0645e-04\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.1189 - val_accuracy: 0.9845 - lr: 7.9389e-04\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.1451 - val_accuracy: 0.9800 - lr: 7.8104e-04\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.1353 - val_accuracy: 0.9830 - lr: 7.6791e-04\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.1189 - val_accuracy: 0.9849 - lr: 7.5452e-04\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 4.5543e-04 - accuracy: 0.9999 - val_loss: 0.1118 - val_accuracy: 0.9848 - lr: 7.4088e-04\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.2268e-04 - accuracy: 1.0000 - val_loss: 0.1168 - val_accuracy: 0.9852 - lr: 7.2700e-04\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.3276e-04 - accuracy: 0.9999 - val_loss: 0.1135 - val_accuracy: 0.9862 - lr: 7.1289e-04\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.2168e-05 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9864 - lr: 6.9857e-04\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 3.2712e-05 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9868 - lr: 6.8406e-04\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 4.9344e-06 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9867 - lr: 6.6937e-04\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 3.0344e-06 - accuracy: 1.0000 - val_loss: 0.1191 - val_accuracy: 0.9868 - lr: 6.5451e-04\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.4284e-06 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9866 - lr: 6.3950e-04\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.9774e-06 - accuracy: 1.0000 - val_loss: 0.1213 - val_accuracy: 0.9868 - lr: 6.2434e-04\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.6234e-06 - accuracy: 1.0000 - val_loss: 0.1225 - val_accuracy: 0.9868 - lr: 6.0907e-04\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.3444e-06 - accuracy: 1.0000 - val_loss: 0.1237 - val_accuracy: 0.9867 - lr: 5.9369e-04\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 1.1168e-06 - accuracy: 1.0000 - val_loss: 0.1249 - val_accuracy: 0.9866 - lr: 5.7822e-04\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.3000e-07 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9867 - lr: 5.6267e-04\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 7.7553e-07 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9868 - lr: 5.4705e-04\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 6.5068e-07 - accuracy: 1.0000 - val_loss: 0.1288 - val_accuracy: 0.9868 - lr: 5.3140e-04\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.4730e-07 - accuracy: 1.0000 - val_loss: 0.1300 - val_accuracy: 0.9868 - lr: 5.1571e-04\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 4.6036e-07 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9867 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 3.8927e-07 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9867 - lr: 4.8429e-04\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 3.2970e-07 - accuracy: 1.0000 - val_loss: 0.1340 - val_accuracy: 0.9868 - lr: 4.6860e-04\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.7902e-07 - accuracy: 1.0000 - val_loss: 0.1353 - val_accuracy: 0.9867 - lr: 4.5295e-04\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 2.3793e-07 - accuracy: 1.0000 - val_loss: 0.1365 - val_accuracy: 0.9866 - lr: 4.3733e-04\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.0297e-07 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9866 - lr: 4.2178e-04\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.7403e-07 - accuracy: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.9866 - lr: 4.0631e-04\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 1.4968e-07 - accuracy: 1.0000 - val_loss: 0.1401 - val_accuracy: 0.9865 - lr: 3.9093e-04\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.2894e-07 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9864 - lr: 3.7566e-04\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.1164e-07 - accuracy: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.9864 - lr: 3.6050e-04\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 9.7003e-08 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9863 - lr: 3.4549e-04\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 8.4571e-08 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9864 - lr: 3.3063e-04\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 7.4024e-08 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9866 - lr: 3.1594e-04\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 6.4793e-08 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9865 - lr: 3.0143e-04\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.7419e-08 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9865 - lr: 2.8711e-04\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.0716e-08 - accuracy: 1.0000 - val_loss: 0.1481 - val_accuracy: 0.9865 - lr: 2.7300e-04\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 4.5071e-08 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.9863 - lr: 2.5912e-04\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 4.0332e-08 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9863 - lr: 2.4548e-04\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 3.6066e-08 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9863 - lr: 2.3209e-04\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 3.2497e-08 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9863 - lr: 2.1896e-04\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.9363e-08 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9863 - lr: 2.0611e-04\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.6683e-08 - accuracy: 1.0000 - val_loss: 0.1523 - val_accuracy: 0.9862 - lr: 1.9355e-04\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 2.4296e-08 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9862 - lr: 1.8129e-04\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.2183e-08 - accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9862 - lr: 1.6934e-04\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.0377e-08 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9862 - lr: 1.5773e-04\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.8843e-08 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9862 - lr: 1.4645e-04\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 1.7400e-08 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9862 - lr: 1.3552e-04\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 1.6198e-08 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9862 - lr: 1.2494e-04\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 1.5122e-08 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9862 - lr: 1.1474e-04\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.4139e-08 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9862 - lr: 1.0492e-04\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.3302e-08 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9862 - lr: 9.5492e-05\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.2636e-08 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9863 - lr: 8.6460e-05\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.2000e-08 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9863 - lr: 7.7836e-05\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 1.1484e-08 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9864 - lr: 6.9629e-05\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 1.1024e-08 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9864 - lr: 6.1847e-05\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.0592e-08 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9864 - lr: 5.4497e-05\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.0247e-08 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9864 - lr: 4.7586e-05\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.9589e-09 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9864 - lr: 4.1123e-05\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.7056e-09 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9864 - lr: 3.5112e-05\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.4424e-09 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9864 - lr: 2.9560e-05\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.2487e-09 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9864 - lr: 2.4472e-05\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.1294e-09 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9864 - lr: 1.9853e-05\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 9.0351e-09 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9865 - lr: 1.5708e-05\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.9407e-09 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9866 - lr: 1.2042e-05\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.8786e-09 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.7867e-09 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 8.7668e-09 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.6948e-09 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.6452e-09 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.5880e-09 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9866 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Emplear un LRS propio, CosineAnnealingScheduler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "lr_max=0.001\n",
    "lr_min=0.00001\n",
    "epochs=100\n",
    "def cosine_annealing(x): # recordemos, x es el número de epoch\n",
    "    lr = lr_max/2 * (1 + math.cos(math.pi * x / epochs))\n",
    "    if lr<lr_min:\n",
    "        lr=lr_min\n",
    "    return lr\n",
    "\n",
    "LRS = LearningRateScheduler(cosine_annealing)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio tipo CosineAnnealing with restarts\n",
    "\n",
    "![cosine annealing restarts](cosinerestart.png)\n",
    "\n",
    "Este scheduler tiene sentido cuanto queremos guardarnos cada uno de los estado del modelo alcanzado en el mínimo LR para luego combinarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2127 - accuracy: 0.9363 - val_loss: 0.1070 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0785 - accuracy: 0.9751 - val_loss: 0.0861 - val_accuracy: 0.9743 - lr: 9.9384e-04\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0505 - accuracy: 0.9838 - val_loss: 0.0910 - val_accuracy: 0.9728 - lr: 9.7553e-04\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0354 - accuracy: 0.9881 - val_loss: 0.0827 - val_accuracy: 0.9768 - lr: 9.4550e-04\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0236 - accuracy: 0.9921 - val_loss: 0.0783 - val_accuracy: 0.9800 - lr: 9.0451e-04\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.1053 - val_accuracy: 0.9753 - lr: 8.5355e-04\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.0965 - val_accuracy: 0.9767 - lr: 7.9389e-04\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0859 - val_accuracy: 0.9813 - lr: 7.2700e-04\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0766 - val_accuracy: 0.9836 - lr: 6.5451e-04\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0912 - val_accuracy: 0.9825 - lr: 5.7822e-04\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0803 - val_accuracy: 0.9846 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.3077e-04 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9862 - lr: 4.2178e-04\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.0690e-04 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9861 - lr: 3.4549e-04\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 6.6774e-05 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9860 - lr: 2.7300e-04\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.3738e-05 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9860 - lr: 2.0611e-04\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 4.6973e-05 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9858 - lr: 1.4645e-04\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.2602e-05 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9859 - lr: 9.5492e-05\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 3.9684e-05 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9859 - lr: 5.4497e-05\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.7990e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9860 - lr: 2.4472e-05\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 3.7147e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9860 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0635 - accuracy: 0.9823 - val_loss: 0.0849 - val_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0778 - val_accuracy: 0.9808 - lr: 9.9384e-04\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0783 - val_accuracy: 0.9830 - lr: 9.7553e-04\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0916 - val_accuracy: 0.9804 - lr: 9.4550e-04\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0910 - val_accuracy: 0.9824 - lr: 9.0451e-04\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.1098 - val_accuracy: 0.9812 - lr: 8.5355e-04\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.1071 - val_accuracy: 0.9808 - lr: 7.9389e-04\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.1165 - val_accuracy: 0.9803 - lr: 7.2700e-04\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.1047 - val_accuracy: 0.9837 - lr: 6.5451e-04\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0975 - val_accuracy: 0.9840 - lr: 5.7822e-04\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.0247e-04 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9847 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 3.7593e-05 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9848 - lr: 4.2178e-04\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 2.9339e-05 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9849 - lr: 3.4549e-04\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.4942e-05 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9849 - lr: 2.7300e-04\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.2000e-05 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 0.9851 - lr: 2.0611e-04\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.9950e-05 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 0.9850 - lr: 1.4645e-04\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.8518e-05 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9850 - lr: 9.5492e-05\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.7570e-05 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9850 - lr: 5.4497e-05\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.6994e-05 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9851 - lr: 2.4472e-05\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.6705e-05 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9851 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.4573e-05 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 0.9855 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 6.8621e-06 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9854 - lr: 9.9384e-04\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.9510e-06 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9858 - lr: 9.7553e-04\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.7943e-06 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9857 - lr: 9.4550e-04\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 2.0846e-06 - accuracy: 1.0000 - val_loss: 0.1104 - val_accuracy: 0.9858 - lr: 9.0451e-04\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.6311e-06 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9858 - lr: 8.5355e-04\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.3004e-06 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9857 - lr: 7.9389e-04\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.0679e-06 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9857 - lr: 7.2700e-04\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 8.9182e-07 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9858 - lr: 6.5451e-04\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 7.5613e-07 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9859 - lr: 5.7822e-04\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 6.5166e-07 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9859 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.7010e-07 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9860 - lr: 4.2178e-04\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 5.0694e-07 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9860 - lr: 3.4549e-04\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.5679e-07 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9859 - lr: 2.7300e-04\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.1863e-07 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9859 - lr: 2.0611e-04\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.8927e-07 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9861 - lr: 1.4645e-04\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.6837e-07 - accuracy: 1.0000 - val_loss: 0.1212 - val_accuracy: 0.9861 - lr: 9.5492e-05\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.5324e-07 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9861 - lr: 5.4497e-05\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.4422e-07 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9860 - lr: 2.4472e-05\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.3949e-07 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9860 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0376 - accuracy: 0.9926 - val_loss: 0.1249 - val_accuracy: 0.9688 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.0948 - val_accuracy: 0.9806 - lr: 9.9384e-04\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.1032 - val_accuracy: 0.9812 - lr: 9.7553e-04\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1089 - val_accuracy: 0.9826 - lr: 9.4550e-04\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.1190 - val_accuracy: 0.9821 - lr: 9.0451e-04\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.1218 - val_accuracy: 0.9830 - lr: 8.5355e-04\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 5.8231e-04 - accuracy: 0.9998 - val_loss: 0.1166 - val_accuracy: 0.9825 - lr: 7.9389e-04\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1190 - val_accuracy: 0.9831 - lr: 7.2700e-04\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1182 - val_accuracy: 0.9841 - lr: 6.5451e-04\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1215 - val_accuracy: 0.9824 - lr: 5.7822e-04\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.1178 - val_accuracy: 0.9839 - lr: 5.0000e-04\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.0575e-04 - accuracy: 0.9999 - val_loss: 0.1151 - val_accuracy: 0.9852 - lr: 4.2178e-04\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 2.1264e-05 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9848 - lr: 3.4549e-04\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.1715e-05 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9850 - lr: 2.7300e-04\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 8.1139e-06 - accuracy: 1.0000 - val_loss: 0.1233 - val_accuracy: 0.9850 - lr: 2.0611e-04\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 6.2432e-06 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9849 - lr: 1.4645e-04\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 5.2218e-06 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9850 - lr: 9.5492e-05\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.6541e-06 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9850 - lr: 5.4497e-05\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.3647e-06 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 0.9850 - lr: 2.4472e-05\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 4.2323e-06 - accuracy: 1.0000 - val_loss: 0.1284 - val_accuracy: 0.9850 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.6179e-06 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9846 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 3.5336e-06 - accuracy: 1.0000 - val_loss: 0.1444 - val_accuracy: 0.9841 - lr: 9.9384e-04\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0457 - accuracy: 0.9888 - val_loss: 0.1150 - val_accuracy: 0.9811 - lr: 9.7553e-04\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.1109 - val_accuracy: 0.9843 - lr: 9.4550e-04\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.1259 - val_accuracy: 0.9827 - lr: 9.0451e-04\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.1171 - val_accuracy: 0.9833 - lr: 8.5355e-04\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 5.1599e-04 - accuracy: 0.9999 - val_loss: 0.1223 - val_accuracy: 0.9844 - lr: 7.9389e-04\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 1.1070e-04 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9849 - lr: 7.2700e-04\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 2.1906e-05 - accuracy: 1.0000 - val_loss: 0.1190 - val_accuracy: 0.9851 - lr: 6.5451e-04\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 1.3544e-05 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9851 - lr: 5.7822e-04\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.0648e-05 - accuracy: 1.0000 - val_loss: 0.1211 - val_accuracy: 0.9852 - lr: 5.0000e-04\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 8.8672e-06 - accuracy: 1.0000 - val_loss: 0.1220 - val_accuracy: 0.9852 - lr: 4.2178e-04\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 7.6011e-06 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 0.9854 - lr: 3.4549e-04\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 6.6872e-06 - accuracy: 1.0000 - val_loss: 0.1235 - val_accuracy: 0.9855 - lr: 2.7300e-04\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 6.0150e-06 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 0.9855 - lr: 2.0611e-04\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 5.5214e-06 - accuracy: 1.0000 - val_loss: 0.1246 - val_accuracy: 0.9855 - lr: 1.4645e-04\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 5.1638e-06 - accuracy: 1.0000 - val_loss: 0.1250 - val_accuracy: 0.9856 - lr: 9.5492e-05\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 4.9181e-06 - accuracy: 1.0000 - val_loss: 0.1252 - val_accuracy: 0.9856 - lr: 5.4497e-05\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 4.7678e-06 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9856 - lr: 2.4472e-05\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 4.6937e-06 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9856 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Emplear un LRS propio, CosineAnnealingScheduler with restarts\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "lr_max=0.001\n",
    "lr_min=0.00001\n",
    "epochs=100\n",
    "\n",
    "def cosine_annealing_with_restarts(x):\n",
    "    lr = lr_max/2 * (1 + math.cos(math.pi * (x % (epochs/5)) / (epochs/5)))\n",
    "    if lr<lr_min:\n",
    "        lr=lr_min\n",
    "    return lr\n",
    "\n",
    "LRS = LearningRateScheduler(cosine_annealing_with_restarts)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Prueba los diferentes LRS sobre el modelo anterior. Emplea la que creas que es la mejor topología de red y optimizador. Emplea early stopping y model checkpoint. Calcula el accuracy sobre el test con el modelo almacenado en el model checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9355\n",
      "Epoch 1: val_accuracy improved from -inf to 0.96592, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.2122 - accuracy: 0.9356 - val_loss: 0.1079 - val_accuracy: 0.9659 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9754\n",
      "Epoch 2: val_accuracy improved from 0.96592 to 0.97408, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0789 - accuracy: 0.9754 - val_loss: 0.0830 - val_accuracy: 0.9741 - lr: 9.9975e-04\n",
      "Epoch 3/100\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0488 - accuracy: 0.9846\n",
      "Epoch 3: val_accuracy did not improve from 0.97408\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0487 - accuracy: 0.9846 - val_loss: 0.0917 - val_accuracy: 0.9722 - lr: 9.9901e-04\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9875\n",
      "Epoch 4: val_accuracy improved from 0.97408 to 0.97558, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0381 - accuracy: 0.9875 - val_loss: 0.0931 - val_accuracy: 0.9756 - lr: 9.9778e-04\n",
      "Epoch 5/100\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9911\n",
      "Epoch 5: val_accuracy improved from 0.97558 to 0.98242, saving model to best_model.h5\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0267 - accuracy: 0.9910 - val_loss: 0.0735 - val_accuracy: 0.9824 - lr: 9.9606e-04\n",
      "=============================\n",
      "best_acc:  0.9824166893959045\n",
      "=============================\n",
      "Test loss: 0.07058686763048172\n",
      "Test accuracy: 0.9790999889373779\n"
     ]
    }
   ],
   "source": [
    "## Solución\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD,Adam,Adagrad\n",
    "\n",
    "batch_size=128\n",
    "epochs=100\n",
    "lr=0.01\n",
    "\n",
    "opt_adam = Adam(learning_rate=lr)\n",
    "\n",
    "best_acc=0.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt_adam,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "LRS = LearningRateScheduler(cosine_annealing)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_val, y_val),\n",
    "                callbacks=[callback, checkpoint, LRS]) \n",
    "\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"best_acc: \", history.history['val_accuracy'][-1])\n",
    "print(\"=============================\")\n",
    "\n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA, estandaricemos la clase MLP\n",
    "\n",
    "Vamos a implementar una clase para ahorrarnos escribir código recurrentemente. \n",
    "Para ello empleamos la **functional api** de Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "class MLP(keras.Model):\n",
    "\n",
    "  def __init__(self, input_size,num_classes,hidden=[128]):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden = []\n",
    "    self.num_h=len(hidden)\n",
    "\n",
    "    for h in hidden:\n",
    "       self.hidden.append(Dense(h, activation='relu'))\n",
    "    self.out = Dense(num_classes, activation='softmax')\n",
    "    \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    \n",
    "    x=self.hidden[0](inputs)\n",
    "    for h in range(1,self.num_h):\n",
    "        x = self.hidden[h](x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "model = MLP(784,10,[1024,512])  ## <-- aquí se instancia el modelo, input, num clases y lista con hidden layers\n",
    "model.build((None,784)) ## Esto es necesario para poder instaciar adecuadamente todos los shapes del grafo de computación\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
